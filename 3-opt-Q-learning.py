import gym
import random
import numpy as np
import rubiks_cube_gym
import seaborn as sns
import matplotlib.pyplot as plt


# Define your Gym environment
env = gym.make('rubiks-cube-222-v0')

#scramble denotes the moves used to move the cube away from its initial state, use "R", "F" and "U" to add moves
scramble="R F U R U R R F"

#the following values keep track of the number of times the solution is reached by the heuristic, and by heuristic-Q
#the solution list is used to determine the avg, best and worst solutions
solutionreached_sa = 0
solutionreached_q = 0
solutionlist = []

#Q-learning parameters
learning_rate = 0.1
discount_factor = 0.9
max_episodes = 10000
max_tillreset = 50
epsilon = 0.1

# Update q-table
def update_q_table(q_table, state, action, reward, learning_rate, discount_factor):
    current_q_value = q_table[state][action]
    updated_q_value = current_q_value + learning_rate * (reward - current_q_value)
    q_table[state][action] = updated_q_value

# Calculate the fitness of the cube state based on how many cubes are out of place
def fitness_evaluation(cube_state, goal_state):

    if cube_state is None:
        return float('inf')  # Return a high fitness if the state is invalid
    fitness = 0

    for i in range(len(env.cube_reduced)):
        if env.cube_reduced[i] != goal_state[i]:
            fitness += 1

    return fitness

# Evaluate the total fitness of a sequence of actions in the environment
# based on how many cubes are out of place compared to the goal state.
def evaluate_sequence(env, sequence, goal_state):

    state = env.reset(scramble=scramble)
    for action in sequence:
        reward_total = 0
        current_state1 = env.cube_state
        # Simulate the cube state change with the action
        observation, reward, done, info = env.step(action)
        reward_total = reward_total + reward  # Accumulate the reward over time
        next_state = env.cube_reduced  # Get the next state
        next_state1 = env.cube_state

        # Calculate the fitness for the next state
        next_fitness = fitness_evaluation(next_state, goal_state)

        # Calculate the reward based on fitness (you can customize this)
        reward = 2.0 / (next_fitness + 1)

        # Update the Q-table with the calculated reward
        update_q_table(q_table, current_state1, action, reward, learning_rate, discount_factor)

        if done:
            break
    return fitness_evaluation(observation, goal_state)


# Define the goal state (customize as needed)
goal_state = "WWWWOOGGRRBBOOGGRRBBYYYY"

import random

def optimize_sequence_using_3_opt(env, goal_state, initial_sequence):

    # Optimize a sequence of actions using 3-opt swapping.

    def apply_3_opt_swap(sequence, i, j, k):

        # Apply the 3-opt swap operation to a sequence of actions at indices i, j, and k.

        new_sequence = (
            sequence[:i] + sequence[i:j + 1][::-1] + sequence[j + 1:k + 1][::-1] + sequence[k + 1:]
        )
        return new_sequence

    # Evaluate the initial sequence
    initial_fitness = evaluate_sequence(env, initial_sequence, goal_state)

    # Initialize the optimized sequence
    optimized_sequence = initial_sequence.copy()

    # Generate a random order of indices (i, j, k)
    indices_order = [(i, j, k) for i in range(len(optimized_sequence) - 2)
                    for j in range(i + 1, len(optimized_sequence) - 1)
                    for k in range(j + 1, len(optimized_sequence))]
    random.shuffle(indices_order)

    for i, j, k in indices_order:
        # Apply 3-opt swap
        new_sequence = apply_3_opt_swap(optimized_sequence, i, j, k)
        # Evaluate the new sequence
        new_fitness = evaluate_sequence(env, new_sequence, goal_state)

        # If the new sequence is better, accept it
        if new_fitness < initial_fitness:
            optimized_sequence = new_sequence
            initial_fitness = new_fitness

        if env.cube_reduced == goal_state:
            print("Solution found! - 3-OPT")
            global solutionreached_sa
            solutionreached_sa = solutionreached_sa + 1
            print(env.cube_reduced)
            # Reduce the sequence size to the point where the goal state is reached
            optimized_sequence = optimized_sequence[:i + 1]
            return optimized_sequence

    # Print the optimized sequence and its fitness
    print(env.cube_reduced)
    print("Size of Optimized Sequence:", len(optimized_sequence))

    return optimized_sequence

# Generate an initial random sequence with 50 moves
initial_sequence = [env.action_space.sample() for _ in range(50)]

# Perform Q-learning using data generated by 3-opt
def q_learning_to_solution(env, goal_state, max_episodes, max_tillreset, learning_rate, discount_factor, epsilon, q_table):

    for episode in range(max_episodes):
        state = env.reset(scramble=scramble)
        done = False
        episode_steps = 0  # Track the number of steps in this episode

        while not done and episode_steps < max_tillreset:
            # Choose an action using epsilon-greedy strategy
            #env.render()
            if np.random.rand() < epsilon:
                action = env.action_space.sample()  # Explore
            else:
                action = np.argmax(q_table[state])  # Exploit


            next_state, reward, done, _ = env.step(action)

            current_fitness = fitness_evaluation(state, goal_state)
            next_fitness = fitness_evaluation(next_state, goal_state)
            reward = 2.0 / (next_fitness + 1)

            # Update the Q-table using the Q-learning update rule
            update_q_table(q_table, state, action, reward, learning_rate, discount_factor)
            state = next_state
            episode_steps += 1

            if done == True:
                print("Solution found! - Q-LEARNING")
                print(env.cube_reduced)
                print(episode_steps)

                global solutionreached_q
                global solutionlist

                solutionreached_q = solutionreached_q + 1
                solutionlist.append(episode_steps)

                break

def analyze_list(numbers):
    # Check if the list is not empty
    if not numbers:
        return None, None, None

    # Sort the list in ascending order
    sorted_numbers = sorted(numbers)

    # Find the highest and lowest values
    highest = sorted_numbers[-1]
    lowest = sorted_numbers[0]

    # Calculate the average
    average = sum(sorted_numbers) / len(sorted_numbers)

    return highest, lowest, average

#Perform the experiment as many times as needed
for _ in range(10):
    num_states = env.observation_space.n
    num_actions = env.action_space.n
    q_table = np.zeros((num_states, num_actions))

    optimized_sequence = optimize_sequence_using_3_opt(env, goal_state, initial_sequence)

    q_learning_to_solution(env, goal_state, max_episodes, max_tillreset, learning_rate, discount_factor, epsilon, q_table)

# Print results
print(f'Times solution found with 3-opt: {solutionreached_sa}')
print(f'Times solution found with Q after 3-opt: {solutionreached_q}')

highest_value, lowest_value, average_value = analyze_list(solutionlist)

print(f'Worst solution: {highest_value} steps')
print(f'Best solution: {lowest_value} steps')
print(f'Average solution: {average_value} steps')